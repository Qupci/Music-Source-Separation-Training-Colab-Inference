{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC6MxtLlx7vN"
      },
      "source": [
        "# Colab inference for ZFTurbo's [Music-Source-Separation-Training](https://github.com/ZFTurbo/Music-Source-Separation-Training/)\n",
        "\n",
        "\n",
        "<font size=1>*made by [jarredou](https://github.com/jarredou) & deton</font>  \n",
        "[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/Q5Q811R5YI)\n",
        "\n",
        "<font size=1>Visit [models list](https://docs.google.com/document/d/17fjNvJzj8ZGSer7c7OFe_CNfUKbAxEh_OBv94ZdRG5c/edit?tab=t.0#heading=h.2vdz5zlpb27h) for their descriptions</font>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vKOCPJkyw9yh"
      },
      "outputs": [],
      "source": [
        "#@markdown #GDrive connection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ScA4L7gmQEjM"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "#@markdown # Install\n",
        "\n",
        "%cd /content\n",
        "!git clone -b colab-inference https://github.com/jarredou/Music-Source-Separation-Training\n",
        "\n",
        "#requirements fix by santilli_\n",
        "req_text = \"\"\"\n",
        "mutagen==1.47.0\n",
        "ml_collections==1.1.0\n",
        "numpy>=1.26.0\n",
        "pandas==2.2.2\n",
        "scipy\n",
        "tqdm\n",
        "segmentation_models_pytorch==0.3.3\n",
        "timm\n",
        "audiomentations\n",
        "pedalboard\n",
        "omegaconf\n",
        "beartype\n",
        "rotary_embedding_torch==0.3.5\n",
        "einops\n",
        "# librosa==0.11.0\n",
        "demucs #==4.0.0\n",
        "# transformers==4.35.0\n",
        "torchmetrics==0.11.4\n",
        "spafe==0.3.2\n",
        "protobuf\n",
        "torch_audiomentations\n",
        "asteroid==0.7.0\n",
        "auraloss\n",
        "torchseg\n",
        "\"\"\"\n",
        "\n",
        "with open(\"Music-Source-Separation-Training/requirements.txt\", \"w\") as f:\n",
        "    f.write(req_text)\n",
        "\n",
        "!mkdir '/content/Music-Source-Separation-Training/ckpts'\n",
        "\n",
        "print('Installing the dependencies... This will take a few minutes')\n",
        "!pip install -r 'Music-Source-Separation-Training/requirements.txt' &> /dev/null\n",
        "\n",
        "print('Installation is done !')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GS-QezQ-RG64"
      },
      "outputs": [],
      "source": [
        "%cd '/content/Music-Source-Separation-Training/'\n",
        "import os\n",
        "import subprocess\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import yaml\n",
        "from urllib.parse import quote\n",
        "\n",
        "class IndentDumper(yaml.Dumper):\n",
        "    def increase_indent(self, flow=False, indentless=False):\n",
        "        return super(IndentDumper, self).increase_indent(flow, False)\n",
        "\n",
        "\n",
        "def tuple_constructor(loader, node):\n",
        "    # Load the sequence of values from the YAML node\n",
        "    values = loader.construct_sequence(node)\n",
        "    # Return a tuple constructed from the sequence\n",
        "    return tuple(values)\n",
        "\n",
        "# Register the constructor with PyYAML\n",
        "yaml.SafeLoader.add_constructor('tag:yaml.org,2002:python/tuple',\n",
        "tuple_constructor)\n",
        "\n",
        "def conf_edit(config_path, chunk_size, overlap):\n",
        "    with open(config_path, 'r') as f:\n",
        "        data = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "\n",
        "    # handle cases where 'use_amp' is missing from config:\n",
        "    if 'use_amp' not in data.keys():\n",
        "      data['training']['use_amp'] = True\n",
        "\n",
        "    data['audio']['chunk_size'] = chunk_size\n",
        "    data['inference']['num_overlap'] = overlap\n",
        "\n",
        "    if data['inference']['batch_size'] == 1:\n",
        "      data['inference']['batch_size'] = 2\n",
        "\n",
        "    print(\"Using custom overlap and chunk_size values:\")\n",
        "    print(f\"overlap = {data['inference']['num_overlap']}\")\n",
        "    print(f\"chunk_size = {data['audio']['chunk_size']}\")\n",
        "    print(f\"batch_size = {data['inference']['batch_size']}\")\n",
        "\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(data, f, default_flow_style=False, sort_keys=False, Dumper=IndentDumper, allow_unicode=True)\n",
        "\n",
        "def download_file(url):\n",
        "    # Encode the URL to handle spaces and special characters\n",
        "    encoded_url = quote(url, safe=':/')\n",
        "\n",
        "    path = 'ckpts'\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    filename = os.path.basename(encoded_url)\n",
        "    file_path = os.path.join(path, filename)\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"File '{filename}' already exists at '{path}'.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        response = torch.hub.download_url_to_file(encoded_url, file_path)\n",
        "        print(f\"File '{filename}' downloaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file '{filename}' from '{url}': {e}\")\n",
        "\n",
        "\n",
        "# Stripped model list in this copy to download from\n",
        "\n",
        "\n",
        "#@markdown # Separation\n",
        "#@markdown #### Separation config:\n",
        "input_folder = '/content/drive/MyDrive/input' #@param {type:\"string\"}\n",
        "output_folder = '/content/drive/MyDrive/output' #@param {type:\"string\"}\n",
        "model = 'CENTER-MDX23C-271'\n",
        "extract_instrumental = 0 #@param {type:\"slider\", min:0, max:2, step:1}\n",
        "export_format = 'wav FLOAT' #@param ['wav FLOAT', 'flac PCM_16', 'flac PCM_24']\n",
        "use_tta = False #@param {type:\"boolean\"}\n",
        "swap_stereo = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown *Roformers custom config:*\n",
        "overlap = 2 #@param {type:\"slider\", min:2, max:40, step:1}\n",
        "chunk_size = \"485100\" #@param [88200, 112455, 132300, 156555, 176400, 352800, 485100, 529200, 588800, 587412, 661500, 749259] {allow-input: true}\n",
        "chunk_size = int(chunk_size)\n",
        "\n",
        "if export_format.startswith('flac'):\n",
        "    flac_file = True\n",
        "    pcm_type = export_format.split(' ')[1]\n",
        "else:\n",
        "    flac_file = False\n",
        "    pcm_type = None\n",
        "\n",
        "if model == 'CENTER-MDX23C-271':\n",
        "    model_type = 'mel_band_roformer'\n",
        "    config_path = '/content/Music-Source-Separation-Training/MelBand Roformer Similarity/config_mel_band_roformer_similarity.yaml'\n",
        "    start_check_point = '/content/Music-Source-Separation-Training/MelBand Roformer Similarity/model_mel_band_roformer_ep_25_sdr_15.0049.ckpt'\n",
        "    !gdown --folder \"https://drive.google.com/drive/folders/1uJP5OQuChCQVY4CVB1Ju3nxBskE-dYzy?usp=drive_link\"\n",
        "\n",
        "supported_extensions = {'.aac', '.aif', '.aiff', '.flac', '.m4a', '.mp3', '.ogg', '.opus', '.wav', '.wv'}\n",
        "input_path = Path(input_folder)\n",
        "\n",
        "if not input_path.exists():\n",
        "    raise FileNotFoundError(f\"Input folder '{input_folder}' does not exist.\")\n",
        "\n",
        "audio_files = sorted(\n",
        "    [path for path in input_path.iterdir() if path.is_file() and path.suffix.lower() in supported_extensions]\n",
        ")\n",
        "\n",
        "if not audio_files:\n",
        "    raise ValueError(f\"No supported audio files found in '{input_folder}'.\")\n",
        "\n",
        "def build_command(file_path):\n",
        "    cmd = [\n",
        "        \"python\",\n",
        "        \"inference.py\",\n",
        "        \"--model_type\", model_type,\n",
        "        \"--config_path\", config_path,\n",
        "        \"--start_check_point\", start_check_point,\n",
        "        \"--input-file\", str(file_path),\n",
        "        \"--store_dir\", output_folder,\n",
        "    ]\n",
        "    if extract_instrumental:\n",
        "        cmd.append(\"--extract_instrumental\")\n",
        "    if flac_file:\n",
        "        cmd.append(\"--flac_file\")\n",
        "        if pcm_type:\n",
        "            cmd.extend([\"--pcm_type\", pcm_type])\n",
        "    elif pcm_type:\n",
        "        cmd.extend([\"--pcm_type\", pcm_type])\n",
        "    if use_tta:\n",
        "        cmd.append(\"--use_tta\")\n",
        "    if swap_stereo:\n",
        "        cmd.append(\"--swap_stereo\")\n",
        "    return cmd\n",
        "\n",
        "def run_inference(file_path):\n",
        "    cmd = build_command(file_path)\n",
        "    print(f\"Starting inference for {file_path.name}\")\n",
        "    result = subprocess.run(cmd, stderr=subprocess.PIPE, text=True)\n",
        "    if result.returncode != 0:\n",
        "        stderr = result.stderr.strip() or 'Unknown error'\n",
        "        raise RuntimeError(f\"Inference failed for {file_path.name}: {stderr}\")\n",
        "    print(f\"Finished inference for {file_path.name}\")\n",
        "\n",
        "# Dispatch inference calls concurrently, up to three workers.\n",
        "max_workers = min(3, len(audio_files))\n",
        "failures = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "    futures = {executor.submit(run_inference, file_path): file_path for file_path in audio_files}\n",
        "    for future in as_completed(futures):\n",
        "        file_path = futures[future]\n",
        "        try:\n",
        "            future.result()\n",
        "        except Exception as error:\n",
        "            print(error)\n",
        "            failures.append(file_path.name)\n",
        "\n",
        "if failures:\n",
        "    print(\"Inference completed with errors:\", \", \".join(failures))\n",
        "else:\n",
        "    print(\"Inference finished for all files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ-4Ilx2XTKd"
      },
      "source": [
        "**INST-Mel-Roformers like v1/1x/2** have switched output file names - files labelled as vocals are instrumentals <br>\n",
        "But e.g. not v1e - so if you uncheck extract_instrumentals for it, only one stem called \"other\" will be rendered, and it will be instrumental.<br><br>\n",
        "**Mel Karaoke by becruily and Duality** models output 2 stems, and don't need \"**extract_instrumental**\" option enabled (inverted stem will rather have worse quality, than the model output, plus you won't end up with 3 output files).\n",
        "<br><br>\n",
        "**TTA** - results in longer separation time, \"it gives a little better SDR score but hard to tell if it's really audible in most cases\". <br> it “means \"test time augmentation\", (...) it will do 3 passes on the audio file instead of 1. 1 pass with be with original audio. 1 will be with inverted stereo (L becomes R, R become L). 1 will be with phase inverted and then results are averaged for final output. ” - jarredou\n",
        "<br><br>\n",
        "**Overlap** - higher means longer separation time. 4 is already balanced value, 2 is fast and some people still won't notice any difference. Normally there's not point going over 8.<br><br>\n",
        "**Chunk_size** - most models use the default 485100 (besides Beta6X - 529200, 6 stems SW - 588800 [882000 will probably crash on free T4], and Amane's 4 stems Large - 661500), and achieves the highest SDR with this value (higher than training chunk), but some people occasionally use also 112455 to get different results.<br><br>\n",
        "\n",
        "If your separation can't start and \"**Total files found: 0**\" is shown, be aware that: <br>1) Input must be a path to a folder containing audio files, not direct path to an audio file<br> 2) The Colab is case aware - e.g. call your folder \"input\" not \"Input\".<br> 3) Check if your Google Drive mounting was executed correctly. Open file manager on the left to check if your drive folder is not empty. If it's the case, force remount with the following line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfv8v8jgihdQ"
      },
      "outputs": [],
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGd6ymALpFQX"
      },
      "source": [
        "4) Consider uploading your files to input folder on GDrive before running the Colab - rarely it may happen that the files in the file manager might be invisible despite refreshing the files view - then you can launch the cell above 2 or 3 times to fix it.\n",
        "<br>\n",
        "\n",
        "**\"Propagation unsuccessful\" error** - click on connect again button in the right top corner (might happen if you delete the envrinment manually and start from the first cell again)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
